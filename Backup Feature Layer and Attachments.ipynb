{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a86808-9987-4893-a468-d2867f78408f",
   "metadata": {},
   "source": [
    "# Attachment and Feature Layer\\Table Backup Script\n",
    "## Required Configuration\n",
    "\n",
    "'''\n",
    ">Configuration below\n",
    ">Presumably this is to be stored in an external config file\n",
    ">Or as notebook parameters depending on use case\n",
    "\n",
    ">Can easily be changed when that is the case.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        fs_list (list): a list of objects containing per feature layer configuration information.\n",
    "        This is the list of layers that will be exported\n",
    "        Contains one or more objects as documented below as out_layer:\n",
    "            out_layer (dict) : object in fs_list:\n",
    "                layer_index (int): the index of the layer you are querying. Might need to experiment to identify\n",
    "                the relevant layer, as service indexes are not 1:1 with layer indexes\n",
    "                table (bool): whether or not the layer in question is a table. Different i/o options for tables (for no reason mostly!)\n",
    "                query_str (str): optional sql query to pass to service to retrieve only a subset\n",
    "                itemid (str): the portal item id of the hosted feature layer being retrieved\n",
    "                gis (GIS): esri portal connection object used for arcgis api for python\n",
    "                oid_field (str): The field in the feature layer designated as the oid. case sensitive (not the alias!)\n",
    "                attach_to_gdb (bool) : True -> attach all exported attachments as blob in the relevant gdb layer\\table, False-> Don't\n",
    "        \n",
    "        username (str): username credentials used to login to portal\n",
    "        password (str): password credentials for passed user\n",
    "        portal (str): portal url of hosted data\n",
    "        manual_retry (bool): bool to denote if user will enter credentials manually if connection fails\n",
    "        save_path (str): root folder to save the relevant data\n",
    "        gdb_name (str): name of gdb to save feature set to\n",
    "        add_date_to_path (bool): save everything in a subset of the save+path using todays date\n",
    "        check_count (int): print the status after every x saves\n",
    "        arcpy.env.overwriteOutput (bool): do you want to overwrite if it exists? no actual error handling if it is no....\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af036fd-26f8-4da0-920c-3190d60a491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "###BEGIN CONFIG\n",
    "#Change Configuration and then run the cell to set them\n",
    "#credentials\n",
    "username = 'USERNAME'\n",
    "password = 'PASSWORD'\n",
    "portal = 'https://www.arcgis.com'\n",
    "manual_retry = True\n",
    "\n",
    "#feature service information\n",
    "fs_list = [{'itemid' : 'ITEMID',\n",
    "            'layer_index' : 0,\n",
    "            'query_str' : \"OBJECTID < 10\",\n",
    "            'table' : False,\n",
    "            'oid_field' : 'OBJECTID',\n",
    "            'attach_to_gdb' : True}]\n",
    "\n",
    "#output information (currently just paths, would need to add module for other save methods)\n",
    "save_outpath = 'E:/Test/facility_test1'\n",
    "gdb_name = 'output.gdb'\n",
    "add_date_to_path = True\n",
    "output_excel_name = 'attachments.xlsx'\n",
    "log_status = True\n",
    "#debug info\n",
    "check_count = 10\n",
    "#Length of time before token expires\n",
    "#Max value is 20160, or 14 days\n",
    "#No current token refresh built into the script, so....\n",
    "token_length = 20160\n",
    "#Multiprocessing Configuration\n",
    "#Set to true to enable multiprocessing\n",
    "#Multiprocessing works when run as script\n",
    "pool_downloads = False\n",
    "#Designate pool size\n",
    "cores = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d34f73-6040-4fd4-949a-2cb23729e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Apr 25 06:29:25 2024\n",
    "\n",
    "@author: 388560\n",
    "\"\"\"\n",
    "\n",
    "from arcgis.gis import GIS\n",
    "from arcgis import features\n",
    "from arcgis.features import FeatureLayerCollection\n",
    "import arcpy\n",
    "import pathlib\n",
    "from datetime import date\n",
    "import requests\n",
    "import pandas as pd\n",
    "import getpass\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import time\n",
    "\n",
    "save_path = save_outpath\n",
    "def generate_gis_object(username = '', password = '', portal = 'https://www.arcgis.com', manual_retry = True, token_length = 20160):\n",
    "    \"\"\" Get GIS Object from portal\n",
    "    \n",
    "    Args:\n",
    "        username (str): username credentials used to login to portal\n",
    "        password (str): password creentials for passed user\n",
    "        portal (str): portal url of hosted data\n",
    "        manual_retry (bool): bool to denote if user will enter credentials manually if connection fails\n",
    "        \n",
    "    Returns:\n",
    "        gis (GIS object): esri portal connection object used for arcgis api for python\n",
    "    \"\"\"\n",
    "    try:\n",
    "        gis = GIS(url = portal, username = username, password = password, expiration = token_length)\n",
    "    except:\n",
    "        if manual_retry is True:\n",
    "            print('Failed to login to portal, please enter Username and Password')\n",
    "            username = input('Username:')\n",
    "            password = getpass.getpass('Password:')\n",
    "            gis = GIS(url = portal, username = username, password = password, expiration = token_length)\n",
    "        else:\n",
    "            pass\n",
    "    return gis\n",
    "\n",
    "    \n",
    "def get_layer(itemid, gis, layer_index = 0, table = False):\n",
    "    \"\"\" Get featureset object of specified layer with optional query\n",
    "    \n",
    "    Args:\n",
    "        layer_index (int): the index of the layer you are querying. Might need to experiment to identify\n",
    "        the relevant layer, as service indexes are not 1:1 with layer indexes\n",
    "        table (bool): whether or not the layer in question is a table. Different i/o options for tables (for no reason mostly!)\n",
    "        query_str (str): optional sql query to pass to service to retrieve only a subset\n",
    "        itemid (str): the portal item id of the hosted feature layer being retrieved\n",
    "        gis (GIS): esri portal connection object used for arcgis api for python\n",
    "        \n",
    "    Returns:\n",
    "        list including:\n",
    "        fs (featureset): esri featureset object. This is the json with layer metadata and attributes used\n",
    "        in many esri services. Note that this is an arcgis api for python fs objct, not an arcpy fs! Although there is a\n",
    "        conversion function available https://developers.arcgis.com/python/api-reference/arcgis.features.toc.html#featureset\n",
    "        name (str): string of the layer name, used for saving\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_layer = gis.content.get(itemid)\n",
    "    if table is False:\n",
    "        layer = feature_layer.layers[layer_index]\n",
    "    else:\n",
    "        layer = feature_layer.tables[layer_index]\n",
    "        \n",
    "    \n",
    "    name = layer.properties['name']\n",
    "    #fs = layer.query(where=query_str)\n",
    "    \n",
    "    return layer\n",
    "\n",
    "\n",
    "def fetch_and_save_attachment(attachment, save_path):\n",
    "    \"\"\" gets information from attachment manager and saves file to relevant path\n",
    "    \n",
    "    Args:\n",
    "        attachment (dict): attachment information from attachment manager\n",
    "        save_path (str): base backup folder to save image\n",
    "        \n",
    "        \"\"\"\n",
    "    record_id = attachment['PARENTOBJECTID']\n",
    "    attachment_id = attachment['ID']\n",
    "    name = attachment['NAME']\n",
    "    image_url = attachment['DOWNLOAD_URL']\n",
    "    image_save_path = f'{save_path}/{record_id}/{attachment_id}'\n",
    "    image_path = pathlib.Path(image_save_path)\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "    img_data = requests.get(image_url).content\n",
    "    with open(f'{image_path}/{name}', 'wb') as handler:\n",
    "        handler.write(img_data)\n",
    "\n",
    "def backup_feature_layer(save_path, fs, gdb_name, name):\n",
    "    \"\"\" Makes sure local path\\gdb exists, then save the feature layer without attachments\n",
    "    \n",
    "    Args:\n",
    "        save_path (str): base backup folder featurer layer\n",
    "        fs (featureset object): feature set that is being saved\n",
    "        gdb_name (str): name of gdb to save backup to\n",
    "        name (str): name of featureclass to save in gdb\n",
    "        \n",
    "        \"\"\"\n",
    "    path = pathlib.Path(save_path)\n",
    "    if arcpy.Exists(rf'{save_path}/{gdb_name}'):\n",
    "        fs.save(rf'{save_path}/{gdb_name}',name)\n",
    "    else:\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        #In classic esri fashion, path vars dont work, just strings\n",
    "        arcpy.management.CreateFileGDB(save_path, gdb_name)\n",
    "        fs.save(rf'{save_path}/{gdb_name}',name)\n",
    "\n",
    "def multi_process(attachments, cores):\n",
    "    for i, attachment in enumerate(attachments):\n",
    "        attachments[i]['save'] = save_path\n",
    "    with Pool(cores) as p:\n",
    "        p.map(fetch_and_save_attachment, attachments)\n",
    "    #pass\n",
    "#As far as I can tell, this is the method to change the over write environment var for arcgis api for python\n",
    "#Which means it is inaccessible when arcpy is not licensed?!\n",
    "#Seems like an oversight unless there is a separate environment\\method Im missing\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "if __name__ == '__main__':   \n",
    "    \n",
    "    #Connect to relevant GIS\n",
    "    gis = generate_gis_object(username = username, password = password, portal = portal, manual_retry = manual_retry, token_length = token_length)\n",
    "    \n",
    "    #Add the date to the output folder location if enabled\n",
    "    if add_date_to_path is True:\n",
    "        today = date.today()\n",
    "        save_path += f\"/{gdb_name.rstrip('.gdb')}_{today.strftime('%m_%d_%Y')}\"\n",
    "    \n",
    "    \n",
    "    #Iterate through list of items provided\n",
    "    for out_layer in fs_list:\n",
    "        failed_attachments = []\n",
    "        save_path +=f\"/{out_layer['itemid']}\"\n",
    "        attachment_list = []\n",
    "        oid_list = []\n",
    "        failed_oid_list = []\n",
    "        #Get the layer in question based on configuration (table, index, etc)\n",
    "        layer = get_layer(itemid = out_layer['itemid'], \n",
    "                                 gis = gis,\n",
    "                                 layer_index = out_layer['layer_index'], \n",
    "                                 table = out_layer['table'])\n",
    "        \n",
    "        #Query the feature based on provided query string\n",
    "        fs = layer.query(where=out_layer['query_str'])\n",
    "        #Inherit name of layer (for output fc)\n",
    "        name = layer.properties['name']\n",
    "        \n",
    "        #Generate list of OIDs\n",
    "        for feature in fs:\n",
    "            oid_list.append(feature.attributes[out_layer['oid_field']])\n",
    "        #Save layer\\table to output gdb location\n",
    "        backup_feature_layer(save_path, fs, gdb_name, name)\n",
    "    \n",
    "        #Create attachment manager object to interact with layer attachments\n",
    "        am = features.managers.AttachmentManager(layer)\n",
    "        #Generate a list of attachments for queried features\n",
    "        attachments = am.search(object_ids = oid_list, return_url = True) \n",
    "        attachment_len = len(attachments)\n",
    "        \n",
    "        \n",
    "        #output list of attachments to xlsx file\n",
    "    \n",
    "        df_out = pd.DataFrame(attachments)\n",
    "        #Add Generated attachment path to df for excel export\n",
    "        #Note this is (currently) manually derived\n",
    "        df_out['Attachment Save Path'] =  f'{save_path}/' + df_out['PARENTOBJECTID'].astype(str) + '/' + df_out['ID'].astype(str) + '/' + df_out['NAME']\n",
    "        #df_out = pd.DataFrame(attachments)\n",
    "        df_out.to_excel(f\"{save_path}/{out_layer['itemid']}{output_excel_name}\")\n",
    "        #Save an excel file with a list of all attachments and their oid\n",
    "        \n",
    "        if pool_downloads == True:\n",
    "            multi_process(attachments, cores)\n",
    "        #Iterate through and output attachments using requests \n",
    "        #(attachment manager download method is VERY SLOW ~10-20 seconds per attachment regardless of size)\n",
    "        #Have considered rewriting as async or parallel operation. Could significantly increase speed even more\n",
    "        else:\n",
    "            for progress, attachment in enumerate(attachments):\n",
    "                retries = 0\n",
    "                if progress % 10 == 0 and log_status == True:\n",
    "                    print(f'Outputting {name} attachment {progress} of {attachment_len}')\n",
    "                attachment_list.append(attachment)\n",
    "                try:\n",
    "                    fetch_and_save_attachment(attachment, save_path)\n",
    "                \n",
    "                #Failed Download Handling\n",
    "                except:\n",
    "                    #Just wait 5 seconds in case you lost internet :)\n",
    "                    time.sleep(5)\n",
    "                    #Try 5 times, then add it to a list and move on\n",
    "                    while retries < 5:\n",
    "                        try:\n",
    "                            fetch_and_save_attachment(attachment, save_path)\n",
    "                            break\n",
    "                        except:\n",
    "                            time.sleep(1)\n",
    "                            print('Fail: ',attachment)\n",
    "                            retries += 1\n",
    "                    failed_attachments.append([attachment,save_path])\n",
    "                    \n",
    "        if len(failed_attachments)>0:\n",
    "            fail_df = pd.DataFrame(failed_attachments)\n",
    "            fail_df.to_excel(f\"{save_path}/{out_layer['itemid']}_failed_attachments.xlsx\")\n",
    "            for failed_output in failed_attachment:\n",
    "                fetch_and_save_attachment(failed_output[0], failed_output[1])\n",
    "                \n",
    "        #If configured, attach the files to the relevant gdb\n",
    "        #With datasets containing large numbers of attachments, this is...unwieldy\n",
    "        #Does not technically match data hierarchy as hosted on AGOL\n",
    "        #I can't really think of any benefit, unless the intent is to immediately republish as a service (in which case this should work well!)\n",
    "        if out_layer['attach_to_gdb'] == True:\n",
    "            print('Attaching exported files to records in gdb')\n",
    "            #Enable attachments for newly generated layer or table\n",
    "            arcpy.management.EnableAttachments(rf'{save_path}/{gdb_name}/{name}')\n",
    "            #Redump metadata to csv (because esri likes it better)\n",
    "            df_out.to_csv(f\"{save_path}/{out_layer['itemid']}.csv\")\n",
    "            #Attach files based on generated layer\\table location, object id field from configuration, newly generated csv file, oid field from csv, file path field from csv\n",
    "            arcpy.management.AddAttachments(rf'{save_path}/{gdb_name}/{name}', \n",
    "                                            f'{out_layer[\"oid_field\"]}', \n",
    "                                            f\"{save_path}/{out_layer['itemid']}.csv\",\n",
    "                                            'PARENTOBJECTID', \n",
    "                                            'Attachment Save Path')\n",
    "            \n",
    "            print('gdb attachments complete')\n",
    "        print(f'{name} Export Complete')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
